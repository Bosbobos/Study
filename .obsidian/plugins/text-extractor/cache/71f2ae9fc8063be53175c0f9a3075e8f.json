{"path":"3 - Disciplines/DeepLearning/9 - Files/lecture02-convnets.pdf","text":"Основы глубинного обучения Лекция 2 Обратное распространение ошибки. Свёрточные сети. Евгений Соколов esokolov@hse.ru НИУ ВШЭ, 2025 Обучение нейронных сетей Опрос Что из этого — формула для шага в градиентном спуске? 1. 𝑤! = 𝑤!\"# + 𝜂 𝛻𝑄 𝑤! 2. 𝑤! = 𝑤!\"# − 𝜂 𝛻𝑄 𝑤!\"# 3. 𝑤! = 𝑤!\"# − 𝜂 𝛻𝑄 𝑤! 4. 𝑤! = 𝑤!\"# + 𝜂 𝛻𝑄 𝑤$ Градиентный спуск • Повторять до сходимости: 𝑤! = 𝑤!\"# − 𝜂 𝛻𝑄 𝑤!\"# Новая точка Размер шага Градиент в предыдущей точке Сходимость • Останавливаем процесс, если 𝑤! − 𝑤!\"# < 𝜀 • Другой вариант: ∇𝑄 𝑤! < 𝜀 • Обычно в глубинном обучении: останавливаемся, когда ошибка на тестовой выборке перестаёт убывать Обучение нейронных сетей • Все слои обычно дифференцируемы, поэтому можно посчитать производные по всем параметрам • 𝑎 𝑥 = 𝐹𝐶% 𝑓 𝐹𝐶# 𝑥 • Где здесь параметры? 𝑥 FC! 𝑓 FC\" 𝑎(𝑥) 𝐿 𝑦, 𝑎 𝑥 𝑦 Обучение нейронных сетей • Все слои обычно дифференцируемы, поэтому можно посчитать производные по всем параметрам • 𝑎 𝑥 = 𝐹𝐶% 𝑓 𝐹𝐶# 𝑥 • Где здесь параметры? 𝑥 FC! 𝑓 FC\" 𝑎(𝑥) 𝐿 𝑦, 𝑎 𝑥 𝑦 Обучение нейронных сетей • Все слои обычно дифференцируемы, поэтому можно посчитать производные по всем параметрам • 𝑎 𝑥 = 𝐹𝐶% 𝑓 𝐹𝐶# 𝑥 1 ℓ 2 &'# ℓ 𝐿 𝑦&, 𝑎 𝑥& → min ) 𝑥 FC! 𝑓 FC\" 𝑎(𝑥) 𝐿 𝑦, 𝑎 𝑥 𝑦 Считаем производные • Для градиентного спуска нужны производные ошибки по параметрам: 𝜕 𝜕𝑤* 𝐿 𝑦&, 𝑎 𝑥&, 𝑤 Считаем производные • Для градиентного спуска нужны производные ошибки по параметрам: 𝜕 𝜕𝑤* 𝑎 𝑥&, 𝑤 − 𝑦& % Считаем производные • Для градиентного спуска нужны производные ошибки по параметрам: 𝜕 𝜕𝑤* 𝑎 𝑥&, 𝑤 − 𝑦& % = 2 𝑎 𝑥&, 𝑤 − 𝑦& 𝜕 𝜕𝑤* 𝑎 𝑥&, 𝑤 как сильно изменится ошибка, если пошевелить 𝑤!? как сильно изменится ошибка, если пошевелить 𝑎 𝑥\", 𝑤 ? как сильно изменится 𝑎 𝑥\", 𝑤 , если пошевелить 𝑤!? Считаем производные 𝜕 𝜕𝑤* 𝑎 𝑥&, 𝑤 − 𝑦& % = 2 𝑎 𝑥&, 𝑤 − 𝑦& 𝜕 𝜕𝑤* 𝑎 𝑥&, 𝑤 • 𝑎 𝑥&, 𝑤 = 10, 𝑦& = 9.99: 2 ∗ 0.01 ∗ + +,! 𝑎 𝑥&, 𝑤 • 𝑎 𝑥&, 𝑤 = 10, 𝑦& = 1: 2 ∗ 9 ∗ + +,! 𝑎 𝑥&, 𝑤 как сильно изменится ошибка, если пошевелить 𝑤!? как сильно изменится ошибка, если пошевелить 𝑎 𝑥\", 𝑤 ? как сильно изменится 𝑎 𝑥\", 𝑤 , если пошевелить 𝑤!? Считаем производные • Для градиентного спуска нужны производные ошибки по параметрам: 𝜕 𝜕𝑤* 𝐿 𝑦&, 𝑎 𝑥&, 𝑤 = @ 𝜕 𝜕𝑧 𝐿 𝑦&, 𝑧 -'. /\", , 𝜕 𝜕𝑤* 𝑎 𝑥&, 𝑤 как сильно изменится ошибка, если пошевелить 𝑤!? как сильно изменится ошибка, если пошевелить 𝑎 𝑥\", 𝑤 ? как сильно изменится 𝑎 𝑥\", 𝑤 , если пошевелить 𝑤!? Считаем производные • Следующая задача — научиться вычислять + +,! 𝑎 𝑥&, 𝑤 Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 Как считать производные? 𝟓 −𝟏 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 1 0 -2 1 0 2 0 1 1 −1 Как считать производные? 𝟓 −𝟏 𝟕 −𝟏 −𝟏𝟑 𝟎 𝟏𝟑 1 0 -2 1 0 2 0 1 1 −1 Как считать производные? 𝟓 −𝟏 𝟕 −𝟏 −𝟏𝟑 𝟎 𝟏𝟑 1 0 -2 1 0 2 0 1 1 −1 Как считать производные? 𝟓 −𝟏 𝟕 −𝟏 −𝟏𝟑 𝟎 𝟏𝟑 1 0 -2 1 0 2 0 1 1 −1 Как считать производные? 𝟓 −𝟏 𝟕 −𝟏 −𝟔 𝟕 𝟏𝟑 1 0 -2 1 1 2 0 1 1 −1 Как считать производные? 𝟓 −𝟏 𝟕 −𝟏 −𝟏𝟑 𝟎 𝟏𝟑 1 0 -2 1 0 2 0 1 1 −1 Как считать производные? 𝟓 −𝟏 𝟕 −𝟐 −𝟏𝟐 𝟎 𝟏𝟐 1 0 -2 2 0 2 0 1 1 −1 Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑎 𝑥 = 𝑝##ℎ# 𝑥 + 𝑝%#ℎ% 𝑥 𝜕𝑎 𝜕𝑝## = ? 𝑝## 𝑝$# Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑎 𝑥 = 𝑝##ℎ# 𝑥 + 𝑝%#ℎ% 𝑥 𝜕𝑎 𝜕𝑝## = ℎ#(𝑥) • Чем больше ℎ#(𝑥), тем сильнее 𝑝## влияет на 𝑎 𝑝## 𝑝$# Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑎 𝑥 = 𝑝##𝑓 𝑣##𝑧# 𝑥 + 𝑣%#𝑧% 𝑥 + 𝑝%#ℎ% 𝑥 𝜕𝑎 𝜕𝑣## = ? 𝑝## 𝑝$# 𝑣## Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑎 𝑥 = 𝑝##𝑓 𝑣##𝑧# 𝑥 + 𝑣%#𝑧% 𝑥 + 𝑝%#ℎ% 𝑥 𝜕𝑎 𝜕𝑣## = 𝜕𝑎 𝜕ℎ# 𝜕ℎ# 𝜕𝑣## 𝑝## 𝑝$# 𝑣## Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## 𝜕𝑎 𝜕𝑤## = ? • Показывает, как сильно изменится 𝑎 при изменении 𝑤## Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## • Как сильно изменится 𝑎 при изменении 𝑤##? • Влияет ли на это 𝑣##? 𝑣## Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## • Как сильно изменится 𝑎 при изменении 𝑤##? • Влияет ли на это 𝑣#%? 𝑣#$ Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## • Как сильно изменится 𝑎 при изменении 𝑤##? • Влияет ли на это 𝑝##? 𝑝## Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## • Как сильно изменится 𝑎 при изменении 𝑤##? • Влияет ли на это 𝑤%%? 𝑤$$ Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## • Как сильно изменится 𝑎 при изменении 𝑤##? • Влияет ли на это 𝑣%%? 𝑣$$ Как считать производные? 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒂 𝒉𝟏 𝒉𝟐 𝑤## 𝜕𝑎 𝜕𝑤## = 𝜕𝑎 𝜕ℎ# 𝜕ℎ# 𝜕𝑧# 𝜕𝑧# 𝜕𝑤## + 𝜕𝑎 𝜕ℎ% 𝜕ℎ% 𝜕𝑧# 𝜕𝑧# 𝜕𝑤## 𝑣$$ Как считать производные? • Мы как бы идём в обратную сторону по графу и считаем производные • Метод обратного распространения ошибки (backpropagation) 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒑 𝒉𝟏 𝒉𝟐 𝒙𝟏 𝒙𝟐 𝒛𝟏 𝒛𝟐 𝒑 𝒉𝟏 𝒉𝟐 𝜕𝑝 𝜕ℎ\" 𝜕𝑝 𝜕𝑥! = 𝜕𝑝 𝜕ℎ! 𝜕ℎ! 𝜕𝑧! 𝜕𝑧! 𝜕𝑥! + 𝜕𝑝 𝜕ℎ\" 𝜕ℎ\" 𝜕𝑧! 𝜕𝑧! 𝜕𝑥! + 𝜕𝑝 𝜕ℎ! 𝜕ℎ! 𝜕𝑧\" 𝜕𝑧\" 𝜕𝑥! + 𝜕𝑝 𝜕ℎ\" 𝜕ℎ\" 𝜕𝑧\" 𝜕𝑧\" 𝜕𝑥! 𝜕𝑝 𝜕𝑧! = 𝜕𝑝 𝜕ℎ! 𝜕ℎ! 𝜕𝑧! + 𝜕𝑝 𝜕ℎ\" 𝜕ℎ\" 𝜕𝑧! 𝜕𝑝 𝜕ℎ! 𝜕𝑝 𝜕ℎ\" 𝜕𝑝 𝜕𝑧\" = 𝜕𝑝 𝜕ℎ! 𝜕ℎ! 𝜕𝑧\" + 𝜕𝑝 𝜕ℎ\" 𝜕ℎ\" 𝜕𝑧\" 3: 2: 1: 𝜕𝑝 𝜕𝑥\" = 𝜕𝑝 𝜕ℎ! 𝜕ℎ! 𝜕𝑧! 𝜕𝑧! 𝜕𝑥\" + 𝜕𝑝 𝜕ℎ\" 𝜕ℎ\" 𝜕𝑧! 𝜕𝑧! 𝜕𝑥\" + 𝜕𝑝 𝜕ℎ! 𝜕ℎ! 𝜕𝑧\" 𝜕𝑧\" 𝜕𝑥\" + 𝜕𝑝 𝜕ℎ\" 𝜕ℎ\" 𝜕𝑧\" 𝜕𝑧\" 𝜕𝑥\" Backprop • Во многие формулы входят одни и те же производные • В backprop каждая частная производная вычисляется один раз — вычисление производных по слою N сводится к перемножению матрицы производных по слою N+1 и некоторых векторовПолносвязные сети для изображений MNISTMNIST • Изображения 28 x 28 • Изображения центрированы • 60.000 объектов в обучающей выборке MNIST • Что может выучить полносвязная сеть? https://mmlind.github.io/Simple_3-Layer_Neural_Network_for_MNIST_Handwriting_Recognition/ MNIST • Каждый нейрон может детектировать заполненность конкретного набора пикселей MNISTMNIST • Если немного сдвинуть цифру, то нейрон уже не будет на неё реагировать https://srome.github.io/Jitter,-Convolutional-Neural-Networks,-and-a-Kaggle-Framework/ Число параметров • 784 входа • Полносвязный слой: 1000 нейронов • Выходной слой: 10 нейронов (по одному на каждый класс) • Весов между входным и полносвязным слоями: (784 + 1)*1000 = 785.000 • Весов между полносвязным и выходным слоями: (1000 + 1) * 10 = 10.010 Число параметров • Можно добиться хорошего качества полносвязными сетями (с аугментацией) • https://arxiv.org/abs/1003.0358 Полносвязные слои для изображений • Очень много параметров • Легко могут переобучиться • Не учитывают специфику изображений (сдвиги, небольшие изменения формы и т.д.) • Один из лучших способов борьбы с переобучением — снижение числа параметровСвёртки Эксперименты со зрительной корой https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912/ Свёртка 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 51 2 3 4 Фильтр * Вход Выход Поэлементное умножение, затем суммирование Свёртка 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 5 9 4 5 7 1 2 3 4 Фильтр * Вход Выход Поэлементное умножение, затем суммирование Свёртка 1 1 0 1 1 0 0 1* = 2 1 1 1 1 1 0 0 1* = 2 1 2 3 0 1 0 0 1* = 1 0 2 3 0 1 0 0 1* = 0 3 0 0 3 1 0 0 1* = 6 5 0 0 5 1 0 0 1* = 10 Свёртка • Операция свёртки выявляет наличие на изображении паттерна, который задаётся фильтром • Чем сильнее на участке изображения представлен паттерн, тем больше будет значение свёртки Максимум свёртки инвариантен к сдвигам 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 2 1 0 0 1* = 2 0 0 0 1 0 0 0 0 * = 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 Max = 2 Max = 2 Не меняется Фильтр Вход Выход Фильтр Вход Выход Свёртки в компьютерном зрении https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 Свёртки в компьютерном зрении https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html Свёртки в компьютерном зрении https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html Свёртка Im 12! 𝑥, 𝑦 = 2 &'\"3 3 2 *'\"3 3 𝐾 𝑖, 𝑗 \tIm &4 𝑥 + 𝑖, 𝑦 + 𝑗 + 𝑏 Свёртка Im 12! 𝑥, 𝑦 = 2 &'\"3 3 2 *'\"3 3 𝐾 𝑖, 𝑗 \tIm &4 𝑥 + 𝑖, 𝑦 + 𝑗 + 𝑏 • Пиксель в результирующем изображении зависит только от небольшого участка исходного изображения (local connectivity) • Веса одни и те же для всех пикселей результирующего изображения (shared weights) Свёртка • Обычно исходное изображение цветное! • Это означает, что в нём несколько каналов (R, G, B) • Учтём в формуле: Im 12! 𝑥, 𝑦 = 2 &'\"3 3 2 *'\"3 3 2 5'# 6 𝐾 𝑖, 𝑗, 𝑐 \tIm &4 𝑥 + 𝑖, 𝑦 + 𝑗, 𝑐 + 𝑏 Свёртка -1 -1 -1 -1 8 -1 -1 -1 -1 * = 𝑊 𝐻 фильтр 𝑊/×\t𝐻/×𝐶01 Результат 𝐶01 Свёртка • Одна свёртка выделяет конкретный паттерн на изображении • Нам интересно искать много паттернов • Сделаем результат трёхмерным: Im 12! 𝑥, 𝑦, 𝑡 = 2 &'\"3 3 2 *'\"3 3 2 5'# 6 𝐾! 𝑖, 𝑗, 𝑐 \tIm &4 𝑥 + 𝑖, 𝑦 + 𝑗, 𝑐 + 𝑏! Число параметров Im 12! 𝑥, 𝑦, 𝑡 = 2 &'\"3 3 2 *'\"3 3 2 5'# 6 𝐾! 𝑖, 𝑗, 𝑐 \tIm &4 𝑥 + 𝑖, 𝑦 + 𝑗, 𝑐 + 𝑏! • Обучается только фильтр • 2𝑑 + 1 % ∗ 𝐶 + 1 ∗ 𝑇 параметров","libVersion":"0.5.0","langs":""}