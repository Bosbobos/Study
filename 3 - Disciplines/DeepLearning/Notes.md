# Lectures
## 5 - Convnet architectures
У AlexNet была проблема, что 90% параметров находилась в FC-части, то есть мы плохо извлекали параметры, и потом жестко с ними работали

VGG переносит больше количество параметров в сверточную часть, чтобы модель была более сбалансированной.

Проблема - чтобы обучить более сложную версию, надо для начала обучить более простую и ей инициализировать следующую. Это долго и сложно - вместо одной нейронки обучаем 5.

А ещё оказалось, что много 3х3 слоёв лучше, чем мало 11х11 слоёв.

Потом выяснилось, что добавление батчнорма позволяет сразу обучать большую модель, так что проблема была в слишком сложном ландшафте функционала ошибки.

Свертка 1 на 1 это как в методе главных компонент мы все каналы одного "пикселя" клеим в один
![[Notes-1759402992239.webp]]
Здесь в целом идет понижение размерности - мы разными способами достаем разные зависимости из параметров. Но это совсем капец и наука свернула не туда.
## 6 Detectors
В R-CNN находим контуры объектов фильтром Собеля (должно быть на семинаре).

AlexNet используется потому что сам R-CNN старый.

Изначально вообще был лютый Франкенштейн.

Не end-to-end = классические методы комп. зрения в градспуск не засунуть.

Пересмотреть самое интересное на где-то 35-й минуте лекции (чертов тинькоф)
### ResNet
Судя по тому, что повышение количества слоев не помогает даже переобучению. Значит backprop не доходит до первых слоев модели (скорее всего). Так что погнали просто к результату слоя добавим то, что пришло к нему не вход.

Свертка со страйдом 2 заменяет макпулинг в более новых моделях (оно понижает размерность!!)

При этом когда размерность меняется (к примеру 128х128х128 -> 64х64х256) мы пространственно сжимаем (берем каждый второй пиксель), а каналы добиваем нулями

Global average pooling на последнем сверточном слое нужен, чтобы картинки на вход можно было подавать любого размера (по крайней мере в теории, на деле работает только с маленькими изменениями типа 200х200->250х250)
## 7 - написано в apple notes
## 9
### Кодировщик: версия 2
Запрос - что мы ищем (к примеру, действие)
Ключ - что это слово может дать (помогает поиску по запросам)
Значение - сама информация слова
### Некст слайд
Деление на корень d нужен чтобы дисперсия оставалась такой же и значения не раздувались.

Один такой кодировщик аналогичен одной свертке с одним фильтром - в соло находит лишь один признак.

Обучаем мы матрицы, при помощи которых мы извлекаем запросы, ключи, значения.

Можно некоторые матрицы шерить между несколькими инстансами SA, это уже на усмотрение конкретной архитектуры.

Несколько SA в параллели - multi-headed self attention.

Add & Normalize - добавляем к результату SA изначальный вектор (знакомый нам residual connection) и затем нормализуем построчно.

Feed forward это полносвязный слой.

Замечания к кодировщику 5: показанный здесь блок это блок трансформера.

Иногда можно нормализацию оставить как здесь, после SA, это postnorm; можно перед, будет prenorm, он прикольнее, вроде (а можно оба).

Иногда в feed forward можно организовать Mixture of Experts (MoE), там каждый вектор прокидываем через несколько полносвязных слоёв и аггрегируем признаки.

Информация о словах хранится именно в параметрах полносвязной части => растить надо их, если хотим хранить больше инфы.

Positional encoding - насильно добавляем к каждому слову информацию о его позиции (прибавляем (арифметически, не конкатенацией) конкретный вектор, который для каждой позиции свой).

RoPE (rotary position embeddings) - в SA ещё и учитываем разницу между ключом одного слова и значением другого. Это происходит через жесткую математику, вращение и комлпексные числа(?). Это используется в итоговом кодировщике 6

### Encoder-decoder attention
Это блок SA, который считает запросы по словам из изначального входа, а ключи и значения на основе текста кодировщика. Так мы подсматриваем в исходный текст.

Кодировкщик-декодировщик сейчас используется редко, обычно обходятся чем-то одним (как декодировщиком в ллмке).

Bert используется для классификации текстов в основном и других таких задач.

В изначальном виде чисто обогащает входные токены энкодерами.

Обучается на совсем другие две задачи - предсказание пропущенного слова, и идут ли два предложения подряд.

Для прикладных задач поверх BERT-а ставится подходящая голова (обычно полносвязная) и обучается уже чисто она.
# Seminars
## 3 - convnet optimization
При обучении dl-моделей всегда сначала используют адам, потом уже начинают подбирать.

Во время обучения графики важны!! Надо глянуть wandb, и мб аналоги без впн, но без особой надежды.

Пока качество на val-e не начало падать, переобучения нет и можно попробовать ещё подержать обучение. 

Аугментация данных нужна потому что для CV-моделей данных почти всегда мало.

Важно, если вводим аугментацию для трейна, не применять её к валу!! (и тесту)

## 4 - 
Transfer learning - берем в качестве весов-инициализаторов веса обученной на чем-то ещё модели

В resnet происходит что-то вроде градиентного бустинга - каждый слой "исправляет ошибки" предыдущего, иными словами постоянно находит что-то более интересное по сравнению с предыдущими, и ДОБАВЛЯЕТ это к предыдущим

## Доп. сем по бэкпропу
Один нейрон позволяет разделить плоскость на две, то есть сказать, с какой стороны от плоскости мы находимся

Нейронка - комбинация из нескольких логрегов

Первыми слоем нейронка создает себе удобные признаки, в середине ищет и создаёт паттерны и под конец уже работает с ними. Она сама занимается feature engineering-ом

Упражнение 6!! Когда считаем кол-во параметров, не забываем про константу на каждом слое

Нейросеть - вычислительный граф. А производная по нему?..

В вычислительном графе каждая стрелка - операция, ведущая от одного входа к одному выходу (мб промежуточному). То есть по этой конкретной операции мы можем спокойно взять производную.

Тогда взять производную от итогового выхода по одному из изначальных входов это просто пройти по графу, собирая все встречающиеся нам стрелки. Если мы просто по ней проходим, то умножаем; если две стрелки сходятся в одну, то складываем.

Вдобавок, если будем считать обычные производные сложных функций, у нас будет много повторяющихся кусков в расчетах. Здесь мы гарантируем, что ничего не считается лишний раз.

backward pass это, фактически, тот же chain rule.
## 7 - детекция и сегментация
Если после сверток используется BatchNorm, можно поставить bias=False у свёртки, т.к. батчнорм потом всё равно на свой биас поменяет.

## 8 - word embeddings
В английском лучше работает стемминг, в русском - лемматизация (библиотекой pymorphy2)

## 9 - RNN
[Очень классный сайт с теорией по NLP](https://lena-voita.github.io/nlp_course.html)

У RNN встречаются проблемы со взрывом градиентов, clip_grad_norm в обучении позволяет с этим бороться. 

## 10 - машинный перевод
Цель ml - заработать денег и обмануть инвесторов.

Первые способы использовали частотный анализ