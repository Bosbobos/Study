# Lectures
## 5 - Convnet architectures
У AlexNet была проблема, что 90% параметров находилась в FC-части, то есть мы плохо извлекали параметры, и потом жестко с ними работали

VGG переносит больше количество параметров в сверточную часть, чтобы модель была более сбалансированной.

Проблема - чтобы обучить более сложную версию, надо для начала обучить более простую и ей инициализировать следующую. Это долго и сложно - вместо одной нейронки обучаем 5.

А ещё оказалось, что много 3х3 слоёв лучше, чем мало 11х11 слоёв.

Потом выяснилось, что добавление батчнорма позволяет сразу обучать большую модель, так что проблема была в слишком сложном ландшафте функционала ошибки.

Свертка 1 на 1 это как в методе главных компонент мы все каналы одного "пикселя" клеим в один
![[Notes-1759402992239.webp]]
Здесь в целом идет понижение размерности - мы разными способами достаем разные зависимости из параметров. Но это совсем капец и наука свернула не туда.
## 6 Detectors
В R-CNN находим контуры объектов фильтром Собеля (должно быть на семинаре).

AlexNet используется потому что сам R-CNN старый.

Изначально вообще был лютый Франкенштейн.

Не end-to-end = классические методы комп. зрения в градспуск не засунуть.

Пересмотреть самое интересное на где-то 35-й минуте лекции (чертов тинькоф)
### ResNet
Судя по тому, что повышение количества слоев не помогает даже переобучению. Значит backprop не доходит до первых слоев модели (скорее всего). Так что погнали просто к результату слоя добавим то, что пришло к нему не вход.

Свертка со страйдом 2 заменяет макпулинг в более новых моделях (оно понижает размерность!!)

При этом когда размерность меняется (к примеру 128х128х128 -> 64х64х256) мы пространственно сжимаем (берем каждый второй пиксель), а каналы добиваем нулями

Global average pooling на последнем сверточном слое нужен, чтобы картинки на вход можно было подавать любого размера (по крайней мере в теории, на деле работает только с маленькими изменениями типа 200х200->250х250)
# Seminars
## 3 - convnet optimization
При обучении dl-моделей всегда сначала используют адам, потом уже начинают подбирать.

Во время обучения графики важны!! Надо глянуть wandb, и мб аналоги без впн, но без особой надежды.

Пока качество на val-e не начало падать, переобучения нет и можно попробовать ещё подержать обучение. 

Аугментация данных нужна потому что для CV-моделей данных почти всегда мало.

Важно, если вводим аугментацию для трейна, не применять её к валу!! (и тесту)

## 4 - 
Transfer learning - берем в качестве весов-инициализаторов веса обученной на чем-то ещё модели

В resnet происходит что-то вроде градиентного бустинга - каждый слой "исправляет ошибки" предыдущего, иными словами постоянно находит что-то более интересное по сравнению с предыдущими, и ДОБАВЛЯЕТ это к предыдущим

## Доп. сем по бэкпропу
Один нейрон позволяет разделить плоскость на две, то есть сказать, с какой стороны от плоскости мы находимся

Нейронка - комбинация из нескольких логрегов

Первыми слоем нейронка создает себе удобные признаки, в середине ищет и создаёт паттерны и под конец уже работает с ними. Она сама занимается feature engineering-ом

Упражнение 6!! Когда считаем кол-во параметров, не забываем про константу на каждом слое

Нейросеть - вычислительный граф. А производная по нему?..

В вычислительном графе каждая стрелка - операция, ведущая от одного входа к одному выходу (мб промежуточному). То есть по этой конкретной операции мы можем спокойно взять производную.

Тогда взять производную от итогового выхода по одному из изначальных входов это просто пройти по графу, собирая все встречающиеся нам стрелки. Если мы просто по ней проходим, то умножаем; если две стрелки сходятся в одну, то складываем.

Вдобавок, если будем считать обычные производные сложных функций, у нас будет много повторяющихся кусков в расчетах. Здесь мы гарантируем, что ничего не считается лишний раз.

backward pass это, фактически, тот же chain rule.