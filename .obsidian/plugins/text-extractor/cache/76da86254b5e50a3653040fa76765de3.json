{"path":"3 - Disciplines/DeepLearning/9 - Files/lecture04-optimization-convnets.pdf","text":"Основы глубинного обучения Лекция 4 Оптимизация в глубинном обучении. Свёрточные архитектуры. Евгений Соколов esokolov@hse.ru НИУ ВШЭ, 2025 Модификации градиентного спуска ПроблемыПроблемыПроблемы • Если у функции «вытянуты» линии уровня, то градиентный спуск требует аккуратного выбора длины шага и будет долго сходиться Momentum ℎ! = 𝛼ℎ!\"# + 𝜂!𝛻𝑄 𝑤!\"# 𝑤! = 𝑤!\"# − ℎ! • ℎ! — «инерция», усреднённое направление движения • 𝛼 — параметр затухания • Как будто шарик, который катится в сторону минимума, очень тяжёлый Momentum ℎ! = 𝛼ℎ!\"# + 𝜂!𝛻𝑄 𝑤!\"# 𝑤! = 𝑤!\"# − ℎ! Без инерции https://distill.pub/2017/momentum/ С инерцией https://distill.pub/2017/momentum/ Nesterov Momentum ℎ! = 𝛼ℎ!\"# + 𝜂!𝛻𝑄 𝑤!\"# − 𝛼ℎ!\"# 𝑤! = 𝑤!\"# − ℎ! • 𝑤!\"# − 𝛼ℎ!\"# — неплохая оценка того, куда мы попадём на следующем шаге Проблема с разреженными данными • Пример: модель над категориальными признаками • Используем one-hot кодирование популярная категория редкая категория • Делаем стохастический градиентный спуск • Через 7 шагов мы сделаем 4 обновления веса популярной категории и 1 обновление веса редкой категории тут уже медленные шаги Проблема с разреженными данными • По разным параметрам мы движемся с разной скоростью • Будет здорово это учитывать — иначе мы обучим разные параметры с разным качеством Проблема с разными масштабами • Допустим, признаки имеют разный масштаб — от единиц до миллионов • Тогда странно шагать по каждому параметру с одинаковой скоростью AdaGrad 𝐺$ ! = 𝐺$ !\"# + 𝛻𝑄 𝑤!\"# $ % 𝑤$ ! = 𝑤$ !\"# − 𝜂! 𝐺$ ! + 𝜖 𝛻𝑄 𝑤!\"# $ • По каждому параметру своя скорость • 𝜂! можно зафиксировать • Длина шага может убывать слишком быстро и привести к ранней остановке RMSProp 𝐺$ ! = 𝛼𝐺$ !\"# + 1 − 𝛼 𝛻𝑄 𝑤!\"# $ % 𝑤$ ! = 𝑤$ !\"# − 𝜂! 𝐺$ ! + 𝜖 \t𝑔!$ • 𝛼 можно взять около 0.9 • Скорость зависит только от недавних шагов Adam 𝑚! \" = 𝛽#𝑚! \"$# + 1 − 𝛽# 𝛻𝑄 𝑤\"$# ! 1 − 𝛽# \" 𝑣! \" = 𝛽%𝑣! \"$# + 1 − 𝛽% 𝛻𝑄 𝑤\"$# ! % 1 − 𝛽% \" 𝑤! \" = 𝑤! \"$# − 𝜂\" 𝑣! \" + 𝜖 \t𝑚! \" • Рекомендации: 𝛽# = 0.9, 𝛽% = 0.999, 𝜖 = 10 $& Adam • Совмещает в себе идеи из метода инерции и RMSProp Dropout Борьба с переобучением • Сокращение числа параметров (свёрточные слои помогают с этим) • Регуляризация • Можно как-то ещё мешать модели подгоняться под обучающую выборку DropoutDropoutDropoutDropoutDropout • Можно определить как слой 𝑑(𝑥) • Параметров нет, единственный гиперпараметр — 𝑝 (вероятность удаления нейрона) • На этапе обучения: 𝑑 𝑥 = 1 1 − 𝑝 𝑚 ∘ 𝑥 (𝑚 — вектор того же размера, что и 𝑥, элемент берутся из распределения Ber(𝑝)) • Деление на 𝑝 — для сохранения суммарного масштаба выходов https://jmlr.org/papers/v15/srivastava14a.html Dropout Пусть все веса единичные 1 1 1 1 4 4 Dropout Пусть все веса единичные 1 1 1 3 3 Надо компенсировать снижение масштаба суммы выходов! Dropout • На этапе обучения: 𝑑 𝑥 = 1 1 − 𝑝 𝑚 ∘ 𝑥 • На этапе применения: 𝑑 𝑥 = 𝑥 В оригинальной статье нет нормировки на этапе обучения, но есть домножение на (1 − 𝑝) на этапе применения Вариант на слайде — inverted dropout (чуть меньше операций во время применения сети) Dropout • Интерпретация: мы обучаем все возможные архитектуры нейросетей, которые получаются из исходной выбрасыванием отдельных нейронов • У всех этих архитектур общие веса • На этапе применения (почти) усредняем прогнозы всех этих архитектурНормализации Covariate shiftCovariate shift • В классическом машинном обучении — изменение распределения данных • Много методов решения Domain adaptation • Объекты по-разному распределены на обучении и на контроле • Идея: взвешивать объекты при обучении 9 &'# ℓ 𝑠& 𝑎 𝑥& − 𝑦& % → min • Большие веса будем ставить объектам, которые похожи на объекты из тестовой выборки Internal covariate shift • В нейронной сети каждый слой обучается на выходах предыдущих слоёв • Если слой в начале сильно меняется, то все следующие слои надо переделывать Internal covariate shiftInternal covariate shift Допустим, веса первого слоя сильно поменялись после градиентного шага Internal covariate shift • Идея: преобразовывать выходы слоёв так, чтобы они гарантированно имели фиксированное распределение https://arxiv.org/pdf/1502.03167.pdf Batch Normalization • Реализуется как отдельный слой • Вычисляется для текущего батча • Оценим среднее и дисперсию каждой компоненты входного вектора: 𝜇' = 1 𝑛 3 !(# ) 𝑥',! 𝜎' % = 1 𝑛 3 !(# ) 𝑥',! − 𝜇' % покоординатно 𝑥!,# — 𝑗-й объект в батче 𝐵 Batch Normalization • Отмасштабируем все выходы: A𝑥),$ = 𝑥),$ − 𝜇) 𝜎) % + 𝜖 • Зададим нужные нам среднее и дисперсию: 𝑧),$ = 𝛾 ∘ A𝑥),$ + 𝛽 обучаемые параметры (векторы, размерность равна размерности входных векторов) Batch Normalization 𝑥!! ⋯ 𝑥!\" ⋮ ⋱ ⋮ 𝑥#! ⋯ 𝑥#\" • 𝑛 — размер батча • 𝑑 — размерность входного вектора Приводим среднее и дисперсию к 𝛽! и 𝛾! Приводим среднее и дисперсию к 𝛽\" и 𝛾\" Batch Normalization Важно: после BatchNorm среднее и дисперсия каждого выхода зависят только от параметров нормализации, но не от параметров прошлых слоёв! Batch Normalization Во время применения нейронной сети: • Те же самые формулы, но вместо 𝜇) и 𝜎) % используем их средние значения по всем батчам Batch Normalization • Обычно вставляется между полносвязным/свёрточным слоём и нелинейностью • Позволяет увеличить длину шага в градиентном спуске • Не факт, что действительно устраняет covariance shift В чём польза от BatchNorm? https://arxiv.org/pdf/1805.11604.pdf В чём польза от BatchNorm? https://arxiv.org/pdf/1805.11604.pdf • Добавим шум после нормализации — хуже не становится! В чём польза от BatchNorm? https://arxiv.org/pdf/1805.11604.pdf • Как связаны градиенты до и после обновления на предыдущих слоях? В чём польза от BatchNorm? https://arxiv.org/pdf/1805.11604.pdf • Функционал ошибки становится более «гладким»!","libVersion":"0.5.0","langs":""}