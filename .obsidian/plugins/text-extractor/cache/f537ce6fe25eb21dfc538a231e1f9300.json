{"path":"3 - Disciplines/DeepLearning/9 - Files/lecture01-intro.pdf","text":"Основы глубинного обучения Лекция 1 Введение в глубинное обучение Евгений Соколов esokolov@hse.ru НИУ ВШЭ, 2025 Чем будем заниматься? Классическое компьютерное зрение 1. Считаем признаки (есть ли усы, какой формы уши, какой длины хвост, …) 2. Обучаем на них градиентный бустинг • Посчитать признаки — целая история Современное компьютерное зрениеКлассическое NLP 1. Подсчитываем статистику, как часто то или иное слово встречается после данного 2. Генерируем следующее слово из этого распределения https://medium.com/analytics-vidhya/making-a-text-generator-using-markov-chains-e17a67225d10 Современное NLPУспехи в глубинном обучении • Изображения и видео • Трёхмерное компьютерное зрение • Тексты (+код, математика…) • Звук и речь • Рекомендательные системы • Агентные системы • … Организационное Про курс • wiki: http://wiki.cs.hse.ru/Основы_глубинного_обучения • Канал: @iad_2025 • Домашние задания • Проверочные работы (неоцениваемые) • Контрольная работа • Письменный экзамен • Автоматы — решим позже Про оценку 𝑂итоговая = 0.4 ∗ ДЗ + 0.3 ∗ КР + 0.3 ∗ Э Примерный план курса • Метод обратного распространения ошибки • Полносвязные сети • Свёрточные сети • Методы оптимизации для глубинного обучения • Работа с последовательностямиЗачем нужны нейронные сети? Предсказание стоимости квартиры • Линейная модель: 𝑎 𝑥 = 𝑤( + 𝑤) ∗ площадь + 𝑤* ∗ этаж +𝑤+ ∗ расстояние\tдо\tметро + ⋯ • Вряд ли признаки не связаны между собой Предсказание стоимости квартиры • Линейная модель с полиномиальными признаками: 𝑎 𝑥 = 𝑤( + 𝑤) ∗ площадь + 𝑤* ∗ этаж +𝑤+ ∗ расстояние\tдо\tметро + 𝑤, ∗ площадь * +𝑤- ∗ этаж * + 𝑤. ∗ расстояние\tдо\tметро * +𝑤/ ∗ площадь ∗ этаж + ⋯ • Может быть сложно интерпретировать модель • Что такое расстояние\tдо\tметро ∗ этаж *? Градиентный бустинг 𝑎0 𝑥 = D 12) 0 𝑏1 𝑥 • Обучение 𝑁-й модели: 1 ℓ D 32) ℓ 𝐿 𝑦3, 𝑎05) 𝑥3 + 𝑏0 𝑥3 → min 6! 7 Градиентный бустинг • Обучение 𝑁-й модели: 1 ℓ D 32) ℓ 𝑏0 𝑥3 − 𝑠3 (0) * → min 6! 7 𝑠3 (0) = − R : :; 𝐿 𝑦3, 𝑧 ;2<!\"# 7$ — сдвиги Кратко о предыдущем курсе • Линейные модели обучаются градиентным спуском, но плохо подходят для поиска сложных закономерностей • Решающие деревья и их композиции дают отличные результаты, но обучать их трудно Нелинейные закономерностиНелинейные закономерностиНелинейные закономерностиНелинейные закономерности 𝑏! 𝑥 = sign(2𝑥\" + 𝑥! − 5)𝑏\" 𝑥 = sign(−2𝑥\" + 𝑥! + 5) Нелинейные закономерности 𝑏! 𝑥 = sign(2𝑥\" + 𝑥! − 5)𝑏\" 𝑥 = sign(−2𝑥\" + 𝑥! + 5) + - - - - + Нелинейные закономерности 𝑏! 𝑥 = sign(2𝑥\" + 𝑥! − 5)𝑏\" 𝑥 = sign(−2𝑥\" + 𝑥! + 5) + - - - - + Если 𝑏! 𝑥 + 𝑏\" 𝑥 < 0, то оранжевый класс Нелинейные закономерности 𝑧! = −2𝑥! + 𝑥\" + 5 𝑧\" = 2𝑥! + 𝑥\" − 5 𝑏! = sign 𝑧! 𝑏\" = sign 𝑧\" 𝑎 = 𝑏! + 𝑏\" [𝑎 < 0] Нейрон https://en.wikipedia.org/wiki/Neuron Нейрон https://en.wikipedia.org/wiki/Neuron 𝑎 𝑥 = ( 45! 6 𝑤4𝑥4 Нелинейные закономерности как бы нейрон 𝑧! = −2𝑥! + 𝑥\" + 5 𝑧\" = 2𝑥! + 𝑥\" − 5 𝑏! = sign 𝑧! 𝑏\" = sign 𝑧\" 𝑎 = 𝑏! + 𝑏\" [𝑎 < 0] Нелинейные закономерности как бы нейронная сеть 𝑧! = −2𝑥! + 𝑥\" + 5 𝑧\" = 2𝑥! + 𝑥\" − 5 𝑏! = sign 𝑧! 𝑏\" = sign 𝑧\" 𝑎 = 𝑏! + 𝑏\" [𝑎 < 0] Граф вычислений (или нейронная сеть) • 𝑥(() — признаки объекта • ℎ) 𝑥 — преобразование («слой») • 𝑥()) — результат 𝑥(8) ℎ! 𝑥(!) Граф вычислений (или нейронная сеть) 𝑥(8) ℎ! ℎ\" … 𝑥(!) 𝑥(\") Граф вычислений (или нейронная сеть) 𝑥(8) ℎ! ℎ\" ℎ: 𝑦 𝑥(!) 𝑥(\") Граф вычислений (или нейронная сеть) 𝑥(8) ℎ! ℎ\" ℎ: 𝑦! ℎ; 𝑦\" ℎ< 𝑦: Полносвязные слои Полносвязный слой (fully connected, FC) • На входе 𝑛 чисел, на выходе 𝑚 чисел • 𝑥), … , 𝑥1 — входы • 𝑧), … , 𝑧H — выходы • Каждый выход — линейная модель над входами 𝑧I = D 32) 1 𝑤I3𝑥3 + 𝑏I Полносвязный слой (fully connected, FC) 𝑧I = D 32) 1 𝑤I3𝑥3 + 𝑏I https://cs231n.github.io/convolutional-networks/ 𝑥! 𝑥\" 𝑥% 𝑧! 𝑧\" 𝑧% 𝑧& Полносвязный слой (fully connected, FC) 𝑧I = D 32) 1 𝑤I3𝑥3 + 𝑏I • 𝑚 линейных моделей, в каждой (𝑛 + 1) параметров • Всего примерно 𝑚𝑛 параметров в полносвязном слое Полносвязный слой (fully connected, FC) 𝑧I = D 32) 1 𝑤I3𝑥3 + 𝑏I • 𝑚 линейных моделей, в каждой (𝑛 + 1) параметров • Всего примерно 𝑚𝑛 параметров в полносвязном слое • Это очень много: если у нас 1.000.000 входных признаков и 1000 выходов, то это 1.000.000.000 параметров • Надо много данных для обучения Важный вопрос в DL Как объединить слои в мощную модель? Нелинейность • Рассмотрим два полносвязных слоя Нелинейность • Рассмотрим два полносвязных слоя 𝑠J = D I2) H 𝑣JI𝑧I + 𝑐J = D I2) H 𝑣JI D 32) 1 𝑤I3𝑥3 + D I2) H 𝑣JI𝑏I + 𝑐J = = D I2) H D 32) 1 𝑣JI𝑤I3𝑥3 + 𝑣JI𝑏I + 1 𝑚 𝑐J • То есть это ничем не лучше одного полносвязного слоя Нелинейность • Нужно добавлять нелинейную функцию после полносвязного слоя 𝑧I = 𝑓 D 32) 1 𝑤I3𝑥3 + 𝑏I Нелинейность 𝑧I = 𝑓 D 32) 1 𝑤I3𝑥3 + 𝑏I Вариант 1: 𝑓 𝑥 = ) )KLMN 57 (сигмоида) Нелинейность 𝑧I = 𝑓 D 32) 1 𝑤I3𝑥3 + 𝑏I Вариант 2: 𝑓 𝑥 = max 0, 𝑥 (ReLU, REctified Linear Unit) Нелинейность https://en.wikipedia.org/wiki/Activation_function НелинейностьНелинейность https://arxiv.org/pdf/2002.05202 Типичная полносвязная сеть 𝑥(8) FC! 𝑓 FC\" 𝑓 … Типичная полносвязная сеть 𝑥(8) FC! 𝑓 FC\" 𝑓 … • На входе признаки • В последнем слое выходов столько, сколько целевых переменных мы предсказываем Теорема Цыбенко Вольное изложение: • Пусть 𝑔 𝑥 — непрерывная функция • Тогда можно построить двуслойную нейронную сеть, приближающую 𝑔 𝑥 с любой заранее заданной точностью То есть двуслойные нейронные сети ОЧЕНЬ мощные! Теорема Цыбенко Вольное изложение: • Пусть 𝑔 𝑥 — непрерывная функция • Тогда можно построить двуслойную нейронную сеть, приближающую 𝑔 𝑥 с любой заранее заданной точностью То есть двуслойные нейронные сети ОЧЕНЬ мощные! Но очень много параметров и очень сложно обучать Резюме • Идея глубинного обучения — совмещение большого количества дифференцируемых слоёв • Слои извлекают сложные признаки из данных • Полносвязные слои — самый простой (и при этом мощный) вариант • Важны нелинейности","libVersion":"0.5.0","langs":""}